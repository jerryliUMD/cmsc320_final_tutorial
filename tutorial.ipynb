{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ff1f944f-71ec-4f48-8fac-c0fb0802d1fb",
   "metadata": {},
   "source": [
    "# Analyzing Factors of Failure and Success in Student Education"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f93743f7",
   "metadata": {},
   "source": [
    "Collaborators: Russell Benjamin (renjamin),  Jerry D. Li (jli103)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4f6ef66",
   "metadata": {},
   "source": [
    "## What is this?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "126a1dde",
   "metadata": {},
   "source": [
    "As the title suggests, this is a tutorial which explores factors affecting one’s ability to continue and succeed in schooling across different age groups. This idea stems from a conversation Jerry and I had where we realized how vastly different our two backgrounds were before beginning our higher education at the University of Maryland. While Jerry spent his younger schooling years in China around the world, I was brought up in public and private schools around the DMV area. These different environments gave us unique challenges which have each shaped us in their own way. After talking through our experiences from grade school until now, we became curious about the different factors that makes a student succeed or fail in their education.\n",
    "\n",
    "Education is widely known to be important, and many would argue essential. It shapes individuals and provides the ability to contribute meaningfully to society. One of its byproducts is it also enables people from completely different backgrounds to explore common interests, like Jerry and I. This got us wondering: what factors interfere with educational pursuit?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d054338",
   "metadata": {},
   "source": [
    "## What is this tutorial's outcome?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13c50ac9",
   "metadata": {},
   "source": [
    "In this tutorial, we’ll aim to explore obstacles in education and show you the entire data science lifecycle from start to finish. This will begin with data collection, then data processing, exploratory data analysis (EDA), building a model, and finally interpreting the results gathered and tying it back to our topic question.\n",
    "\n",
    " After this tutorial, you’ll be able to:\n",
    "- test\n",
    "- test2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c9d8ecc",
   "metadata": {},
   "source": [
    "## Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90bee056-274c-4d8e-890e-b8ab73ca1793",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import warnings\n",
    "import re\n",
    "import seaborn as sns #pip install seaborn\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.offsetbox import OffsetImage, AnnotationBbox\n",
    "from IPython.display import Image, display, HTML #pip install ipython\n",
    "\n",
    "# Imports for Step 4\n",
    "import pydotplus\n",
    "from sklearn.tree import DecisionTreeClassifier, export_graphviz\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcc52f42",
   "metadata": {},
   "source": [
    "## Step 1: Data Collection\n",
    "In this step, we will gather and comprise interesting data to explore through our tutorial. <br>\n",
    "Source links: (for the csv files, check out this repository...)<br>\n",
    "[1]https://ourworldindata.org/child-maltreatment-and-educational-outcomes <br>\n",
    "[2]https://www.kaggle.com/datasets/jessemostipak/college-tuition-diversity-and-pay <br>\n",
    "[3]https://nces.ed.gov/programs/coe/indicator/cpb/college-enrollment-rate#:~:text=The%20overall%20college%20enrollment%20rate%20of%2018%2D%20to%2024%2Dyear,%2D%20or%204%2Dyear%20institutions <br>\n",
    "[4]https://www.kaggle.com/datasets/shariful07/student-mental-health <br>\n",
    "[5]https://archive-beta.ics.uci.edu/dataset/697/predict+students+dropout+and+academic+success <br>\n",
    "[6]https://nces.ed.gov/programs/coe/indicator/a01/violent-deaths-and-shootings <br>\n",
    "[7]https://research.com/education/college-drug-abuse-statistics <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f568f29d-6092-408b-b03a-59436d1c79cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_children_work = pd.read_csv('working-children-out-of-school-ages-7-14-vs-hours-worked-by-children-ages-7-14.csv')\n",
    "df_usa_salary_potential = pd.read_csv('usa_salary_potential.csv')\n",
    "df_usa_college_enroll_rate = pd.read_csv('usa_college_enroll_rate.csv')\n",
    "df_usa_college_enroll_rate_enthnicity = pd.read_csv('usa_college_enroll_rate_ethnicity.csv')\n",
    "df_malaysia_student_mental_health = pd.read_csv('malaysia_student_mental_health.csv')\n",
    "df_usa_youth_violence_history = pd.read_csv('usa_youth_violence_history.csv')\n",
    "df_drug_abuse_reasons = pd.read_csv('drug_abuse_top_reasons.csv')\n",
    "df_europe_students = pd.read_csv('europe_college_student_data.csv', sep=';')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ccfddb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#If you want to see the whole table without the '...', you can use the following.\n",
    "#pd.set_option('display.max_columns', None)  # Show all columns\n",
    "#pd.set_option('display.max_rows', None)     # Show all rows\n",
    "\n",
    "#If this is too much data, you can reset back to default (recommended).\n",
    "#pd.reset_option('display.max_columns')\n",
    "#pd.reset_option('display.max_rows')\n",
    "\n",
    "def showTable(df, name):\n",
    "    display(HTML(f\"<div style='text-align:center;'><h4>{name}</h4></div>\"))\n",
    "    if name is not None:\n",
    "        display(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "874842aa",
   "metadata": {},
   "source": [
    "Brief: Children working hours will be the data from various countries. Source organization https://ourworldindata.org/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c855ed9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#showTable(df_children_work, 'Working children out of school vs. hours worked by children (age 7-14)')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae9fee64",
   "metadata": {},
   "source": [
    "Brief: Perhaps money is one thing that attracts people to go to universities. It is one of the motivations that drive students to persue higher education. Source organization https://www.kaggle.com/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e597daa",
   "metadata": {},
   "outputs": [],
   "source": [
    "#showTable(df_usa_salary_potential, 'Potential Salary for College Graduates (USA)')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9dc94ab",
   "metadata": {},
   "source": [
    "Brief: We would like to see college enrollment rate and enthnicity rate because knowing this could help us understand more about students' identity and see part of the college environment. Since the United States is a culturally diverse country, student diversity will be an interesting factor to consider. You can exlpore more details on your own. Source organization https://nces.ed.gov/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "981a6833",
   "metadata": {},
   "outputs": [],
   "source": [
    "#showTable(df_usa_college_enroll_rate, 'USA College Enroll Rate 2010 - 2021 for ages 18 to 24')\n",
    "#showTable(df_usa_college_enroll_rate_enthnicity, 'USA College Enroll Rate 2010 - 2021 Ethnicity ages 18 to 24')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db8611e5",
   "metadata": {},
   "source": [
    "Brief: While students are in school, mental health could be an issue. Maybe it is because they want to work hard to get a good job, or to achieve something. The stress level in different courses could be an indicator of this. Here, we found data from International Islamic University Malaysia in year 2020 to show case this. (We are interested in age, course, and mental state.) Source organization https://www.kaggle.com/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "174e3a9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#showTable(df_malaysia_student_mental_health, 'Student Mental Health in International Islamic University Malaysia in year 2020')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c497ba87",
   "metadata": {},
   "source": [
    "Brief: We would also like to explore the violence from age 5 to 18. While the datasets for mental health and violence we have here are completely different from each other, we thought that since violence will get announced and spread across media, like school shootings in the U.S, so people might get worried or panic. From the statistic point of view, this dataset from the National Center for Education could let us see the number of violences. From the emotion point of view, we could 'start' to see students' view of violence. For more details, you would need to explore on your own. Source organization https://nces.ed.gov/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3c467f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#showTable(df_usa_youth_violence_history, 'Violent Deaths at School and Away From School and School Shootings in USA (age 5-18)')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e33e322c",
   "metadata": {},
   "source": [
    "Brief: Drug abuse. This is a direct cause to education performance in common sense. Because it became common sense, we would like to know why. Here is a student dataset from the University of Sao Paulo to show this. Source organization https://research.com/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4e60c25",
   "metadata": {},
   "outputs": [],
   "source": [
    "#showTable(df_drug_abuse_reasons, 'Common Reasons for Drug Abuse')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17e75f0d",
   "metadata": {},
   "source": [
    "Brief: We found a dataset that includes information of European college students. Source organization https://archive-beta.ics.uci.edu/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5166a82c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#showTable(df_europe_students, 'Europe College Student Info')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4f37d6f",
   "metadata": {},
   "source": [
    "Still alive? Keep reading."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08969e5d",
   "metadata": {},
   "source": [
    "## Step 2: Data Processing\n",
    "Many times, the data we got is messy. In this process, we will clean some data. Check for NaN values and duplicate rows first. Drop uneccessary columns, and maybe change the column name if needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fce556e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#df.columns\n",
    "def peekData(df):\n",
    "    print('---------------------------------------------------')\n",
    "    print('Table NaN values count for each column\\n') \n",
    "    print(df.isna().sum(), '\\n')\n",
    "    print('Table duplicated rows count', df.duplicated().sum(), '\\n')\n",
    "    print(df.info())\n",
    "    print('---------------------------------------------------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "572d52b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#peekData(df_children_work)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9baa88fe",
   "metadata": {},
   "source": [
    "Observed: For the children work dataset, there are quite a lot of NaN values, so doing something like imputation won't work. In this case, we are interested in Entity, year, population, and child work hours. So, we can drop the rows that has NaN values in these columns to then see what result we get. Also, the column name is too long, so we will shorten the name."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cafe4b77",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Don't need Continent and Code column, drop it.\n",
    "df_children_work.drop(columns=['Continent'], inplace=True)\n",
    "\n",
    "df_children_work.rename(\n",
    "    columns={'Entity': 'Country',\n",
    "             'Children in employment, work only (% of children in employment, ages 7-14)': 'Work_only_pct', \n",
    "             'Average working hours of children, study and work, ages 7-14 (hours per week)': 'Work_avg',\n",
    "             'Population (historical estimates)': 'Population'}, \n",
    "    inplace=True\n",
    ")\n",
    "\n",
    "df_children_work[(df_children_work['Year'] >= 1999) & (df_children_work['Year'] <= 2016)]\n",
    "\n",
    "df_children_work = df_children_work.dropna(subset=['Work_only_pct', 'Work_avg', 'Population'])\n",
    "\n",
    "#if you want to double check, uncomment this line.\n",
    "#peekData(df_children_work) \n",
    "\n",
    "#df_children_work.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36d37914",
   "metadata": {},
   "outputs": [],
   "source": [
    "#peekData(df_usa_salary_potential)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdfc116d",
   "metadata": {},
   "source": [
    "Observed: For the USA college graduate postential salary, we care about the pay, so we will drop the irrelevant column \"make_world_better_percent\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b5f3365",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_usa_salary_potential = df_usa_salary_potential.drop(columns=['make_world_better_percent'])\n",
    "df_usa_salary_potential = df_usa_salary_potential.drop_duplicates(keep='last') #keep only one of the duplicated rows.\n",
    "df_usa_salary_potential.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "764bdfd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#peekData(df_usa_college_enroll_rate)\n",
    "#peekData(df_usa_college_enroll_rate_enthnicity)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d39409ce",
   "metadata": {},
   "source": [
    "Observed: From the US college enroll and ethnicity rate table, we will get rid of the NaN values and then the last 2 rows (we can see why from Step 1 Data Collection). The column names look clean and the overall table is small. Easy to process :)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73718352",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_usa_college_enroll_rate_enthnicity = df_usa_college_enroll_rate_enthnicity.copy()\n",
    "df_usa_college_enroll_rate = df_usa_college_enroll_rate.iloc[:-2, :-1] #get rid of last 2 rows and last column\n",
    "df_usa_college_enroll_rate_enthnicity = df_usa_college_enroll_rate_enthnicity.iloc[:-2,:-1]\n",
    "\n",
    "df_usa_college_enroll_rate_enthnicity.rename(\n",
    "    columns = {\n",
    "        'Race/ethnicity': 'Race',\n",
    "        '2010-Standard Error': '2010-std-error',\n",
    "        '2021-Standard Error': '2021-std-error'\n",
    "    },\n",
    "    inplace = True\n",
    ")\n",
    "\n",
    "#uncomment to double check\n",
    "display(df_usa_college_enroll_rate.head())\n",
    "display(df_usa_college_enroll_rate_enthnicity.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ef81dac",
   "metadata": {},
   "outputs": [],
   "source": [
    "peekData(df_malaysia_student_mental_health)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bce5e543",
   "metadata": {},
   "source": [
    "Observed: In the student mental health dataset, since there is only 1 missing value in the age column, probablly missing at random. We can take the age mean to fill the NaN value. Also, I would change the column names. The timestamp here does not interest us because it is only 2-3 months data in year 2020 (not wide enough), thus drop it. We will be interested in the servey question answers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "600aad7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Filling age with mean \n",
    "df_malaysia_student_mental_health[\"Age\"].fillna(round(df_malaysia_student_mental_health[\"Age\"].mean()),inplace=True)\n",
    "\n",
    "#get rid of not interested columns\n",
    "df_malaysia_student_mental_health.drop(columns=['Timestamp', 'Choose your gender', 'Marital status', 'Your current year of Study'], inplace=True)\n",
    "\n",
    "#rename long column names\n",
    "df_malaysia_student_mental_health.rename(\n",
    "    columns = {\n",
    "        'What is your course?': 'Course', \n",
    "        'What is your CGPA?': 'CGPA', \n",
    "        'Do you have Depression?': 'Depression', \n",
    "        'Do you have Anxiety?': 'Anxiety',\n",
    "        'Do you have Panic attack?': 'Panic_attack', \n",
    "        'Did you seek any specialist for a treatment?': 'Treatment'}, \n",
    "    inplace = True\n",
    ")\n",
    "\n",
    "# Convert \"yes\" and \"no\" to 1 and 0\n",
    "df_malaysia_student_mental_health['Depression'] = df_malaysia_student_mental_health['Depression'].apply(lambda x: 1 if x == 'Yes' else 0)\n",
    "df_malaysia_student_mental_health['Anxiety'] = df_malaysia_student_mental_health['Anxiety'].apply(lambda x: 1 if x == 'Yes' else 0)\n",
    "df_malaysia_student_mental_health['Panic_attack'] = df_malaysia_student_mental_health['Panic_attack'].apply(lambda x: 1 if x == 'Yes' else 0)\n",
    "df_malaysia_student_mental_health['Treatment'] = df_malaysia_student_mental_health['Treatment'].apply(lambda x: 1 if x == 'Yes' else 0)\n",
    "\n",
    "df_malaysia_student_mental_health['CGPA'] = df_malaysia_student_mental_health['CGPA'].str.replace(' ', '').str.replace('-', ' - ')\n",
    "df_malaysia_student_mental_health['Course'] = df_malaysia_student_mental_health['Course'].str.lower().str.replace(' ', '')\n",
    "\n",
    "df_malaysia_student_mental_health.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9d16aa5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#peekData(df_usa_youth_violence_history)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a284962a",
   "metadata": {},
   "source": [
    "Observed: From the US youth violence dataset, we drop the 'Unnamed:4' column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1756d252",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_usa_youth_violence_history.drop(columns=['Unnamed: 4'], inplace=True)\n",
    "\n",
    "df_usa_youth_violence_history.rename(\n",
    "    columns={\n",
    "        'Student, staff, and other nonstudent school-associated violent deaths': 'Other_violent_deaths',\n",
    "        'Homicides of youth ages 5 –18 at school': 'Homicides',\n",
    "        'Suicides of youth ages 5 –18 at school': 'Suicides'\n",
    "    },\n",
    "    inplace=True\n",
    ")\n",
    "\n",
    "df_usa_youth_violence_history.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3532f069",
   "metadata": {},
   "source": [
    "Observed: From the drug abuse table, we will rename the features for clearer interpretation. We will also parse the percent feature values of their % signs for easier plotting later on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72cb1616",
   "metadata": {},
   "outputs": [],
   "source": [
    "# peekData(df_drug_abuse_reasons)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de36ec42",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_drug_abuse_reasons.rename(columns={'Category' : 'Reason'}, inplace=True)\n",
    "df_drug_abuse_reasons.rename(columns={'Top Reasons For Drug Abuse Among College Students' : 'Percent'}, inplace=True)\n",
    "\n",
    "df_drug_abuse_reasons['Percent'] = df_drug_abuse_reasons['Percent'].str.replace('%', '').astype(float)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e2474b2",
   "metadata": {},
   "source": [
    "Observed: For the Europe students dataset, we will encode our three label categories as 0, 1, or 2 for easier processing in our ML section (Step 4). We will also drop a handful of features to hone in on those of interest for later visualizations. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52c40d4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# peekData(df_europe_students)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4821d3da",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_europe_students['Target'] = df_europe_students['Target'].map({\n",
    "    'Dropout': 0,\n",
    "    'Enrolled': 1,\n",
    "    'Graduate': 2,\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91ba337e",
   "metadata": {},
   "outputs": [],
   "source": [
    "kept_features = ['Marital status', 'Application mode', 'Application order', 'Course', 'Daytime/evening attendance',\n",
    "            'Previous qualification', 'Nacionality', \"Mother's qualification\", \"Father's qualification\", \"Mother's occupation\",\n",
    "            \"Father's occupation\", 'Displaced', 'Educational special needs', 'Debtor',\n",
    "            'Tuition fees up to date', 'Gender', 'Scholarship holder', 'Age at enrollment', 'International',\n",
    "            'Unemployment rate', 'Inflation rate', 'GDP', 'Curricular units 1st sem (approved)',\n",
    "            'Curricular units 1st sem (grade)', 'Curricular units 2nd sem (approved)',\n",
    "            'Curricular units 2nd sem (grade)','Target']\n",
    "\n",
    "df_europe_students = df_europe_students[kept_features]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1660a31",
   "metadata": {},
   "source": [
    "Observed: Good. For the Europe Student dataset, there is no need to clean the data!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ce9a385",
   "metadata": {},
   "source": [
    "## Step 3: Exploratory Data Analysis(EDA) & Visualization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f2c95ec",
   "metadata": {},
   "source": [
    "### Children Labor Hours dataset EDA"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e5442b3",
   "metadata": {},
   "source": [
    "The data for children work may not be perfect to visualize. However, let's first see the overall view, and then do a scatter plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13f1f487",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_children_work = df_children_work.sort_values(by='Year')\n",
    "\n",
    "#Uncomment these to see details\n",
    "#display(df_children_work)\n",
    "years = df_children_work['Year'].unique()\n",
    "display(df_children_work[['Work_only_pct', 'Work_avg', 'Population']].describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "774fed41",
   "metadata": {},
   "source": [
    "Let's do a visualization. We could try to do a 3D scatter plot, but doing a 2D plot will look more pleasing. First normalize the population. Although I care most about children work hours, it is also interesting to see the population at the same time. In seaborn, do sizes=(20, 300), which is like doing normalization. Why not standardize? Because I want to emphasize the proportions of the population, so normalization is good. If you want to emphasize the distribution of values, then standarization is good."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da08afa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter out the specific warning\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning, message=\"The figure layout has changed to tight\")\n",
    "\n",
    "# Create a FacetGrid of scatter plots\n",
    "g = sns.FacetGrid(df_children_work, col='Year', col_wrap=5, height=3, sharex=False, sharey=False)\n",
    "g.map_dataframe(\n",
    "    sns.scatterplot,\n",
    "    x='Work_avg',\n",
    "    y='Work_only_pct',\n",
    "    size='Population',\n",
    "    hue='Country',\n",
    "    sizes=(20, 200),\n",
    "    alpha=0.7,\n",
    ")\n",
    "\n",
    "g.set_titles(col_template=\"Year {col_name}\")\n",
    "g.set_axis_labels('Work Average (hr/week)', 'Work Only Percentage')\n",
    "g.fig.suptitle('Scatter Plots: Work vs. Work Only Percentage by Year for Children age 7-14', y=1.02)\n",
    "plt.subplots_adjust(top=0.9, hspace=0.5) \n",
    "\n",
    "#let's track some countries\n",
    "track_countries = ['Nigeria', 'Gambia', 'Bangladesh', 'Kenya', 'Bolivia']\n",
    "\n",
    "# Iterate through subplots and add annotations\n",
    "for ax in g.axes.flat:\n",
    "    year = ax.get_title().split(' ')[-1]\n",
    "    data_year = df_children_work[df_children_work['Year'] == int(year)]\n",
    "    for _, row in data_year.iterrows():\n",
    "        if row['Country'] in track_countries:\n",
    "            annotation_text = f\"{row['Code']}\"  # Combine country and year\n",
    "            ax.annotate(annotation_text, (row['Work_avg'], row['Work_only_pct']),\n",
    "                        textcoords=\"offset points\", xytext=(0, 2), ha='center')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3401dd37",
   "metadata": {},
   "source": [
    "Explain: For each country there is a color. We can see from the plot, there are so many colorful \"bubbles\" that represent the population proportion. Also, not all countries have children work data for each year. So, the overall trend of the data is inexplicable. Places like KEN, BGD, and GMB showed some interesting data. <br> \n",
    "\n",
    "For KEN, in year 2000, the work average is about 9.5 hrs and work only percentage is around 14 percent; in year 2009, the number for work avg jumped to 32 hrs and work only pct jumped to about 38 pct. <br>\n",
    "\n",
    "For BGD, in year 2006, the work avg is 10 hrs and work only pct is 40; in year 2013, the work avg increased to 30 hrs and work only pct is 60! <br>\n",
    "\n",
    "For GMB, in year 2008, the work avg is about 13 hrs and work pnly pct is 26; in year 2015, the work avg decrease to roughly 10.5 hrs per week but work only pct is 40 pct plus!<br>\n",
    "\n",
    "We are showing this data to you because we think children working between age 7 - 14 could be an indication that their family is somehow financially unstable, which could lead the children to think that making money is worth more than education. Maybe this is inevitable for people born as being the n_th poor generation under some political system, carrying inequality arguments with them. By seeing the increase and decrease in work only pct, we start to wonder if people can see the value in education. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c413d1a",
   "metadata": {},
   "source": [
    "### USA College Potential Salaries by State dataset EDA"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1464598",
   "metadata": {},
   "source": [
    "To see people's motivation in education, one of the things is money. We found a dataset that can show the potential salary for some USA colleges after students graduate. Let's go. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e879441e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We are more interested in the bigger picture, such as the State Data in the USA.\n",
    "# We will extract some USA states we thought could be interesting to see since the education quality is well known.\n",
    "\n",
    "targets = ['Maryland', 'Colorado',  'Massachusetts', 'Virginia', 'New-York']\n",
    "\n",
    "# Calculate mean values for each state\n",
    "mean_values = df_usa_salary_potential.groupby('state_name')[['early_career_pay', 'mid_career_pay', 'stem_percent']].mean().reset_index()\n",
    "\n",
    "# Let's see some colorful plots\n",
    "fig, axes = plt.subplots(nrows=1, ncols=3, figsize=(30, 8))\n",
    "\n",
    "\n",
    "bar_colors = ['skyblue' if state not in targets else 'darkblue' for state in mean_values['state_name']]\n",
    "axes[0].bar(mean_values.index, mean_values['early_career_pay'], color=bar_colors)\n",
    "axes[0].set_title('Mean Early Career Pay')\n",
    "axes[0].set_ylabel('Salary')\n",
    "axes[0].set_xticks(mean_values.index)\n",
    "axes[0].set_xticklabels(mean_values['state_name'], rotation=90, ha='right')\n",
    "\n",
    "bar_colors = ['lightgreen' if state not in targets else 'darkgreen' for state in mean_values['state_name']]\n",
    "axes[1].bar(mean_values.index, mean_values['mid_career_pay'], color=bar_colors)\n",
    "axes[1].set_title('Mean Mid Career Pay')\n",
    "axes[1].set_ylabel('Salary')\n",
    "axes[1].set_xticks(mean_values.index)\n",
    "axes[1].set_xticklabels(mean_values['state_name'], rotation=90, ha='right')\n",
    "\n",
    "bar_colors = ['lightcoral' if state not in targets else 'darkred' for state in mean_values['state_name']]\n",
    "axes[2].bar(mean_values.index, mean_values['stem_percent'], color=bar_colors)\n",
    "axes[2].set_title('Mean STEM %')\n",
    "axes[2].set_ylabel('Percent')\n",
    "axes[2].set_xticks(mean_values.index)\n",
    "axes[2].set_xticklabels(mean_values['state_name'], rotation=90, ha='right')\n",
    "\n",
    "# Show plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "589b1461",
   "metadata": {},
   "source": [
    "Explain: the mean early potential salary for fresh college graduates are mostly in the range [40000, 55000]. Interestingly, California State has the highest average potential pay of all time, yet lower STEM percentage than New-York. By looking at our target states (the dark highlighted bars), the career pay for Maryland and Virginia colleges are about the same (little less than 100k); Maryland beats Virginia with the STEM percentages being higher. Massechusetts have a pretty high range from each of the plot [~80500, ~115000, ~27 pct]. Colorado is relatively lower than other states for salaries. While some states in the USA looks attracting to us, we encourage you to explore this financial aspect because it could be insightful. It seems like going to college can make descent money because we can learn something we couldn't on our own. To extend this idea, let's learn about college diversity.<br>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "381677f4",
   "metadata": {},
   "source": [
    "### USA College Enrollment rate & Ethnicity Enrollment Rate dataset EDA"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b59b0aa",
   "metadata": {},
   "source": [
    "We will look at the USA college enrollment rate data to see how college enrollment could reflect on something. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bd35005",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up the plot\n",
    "plt.figure(figsize=(15, 6))\n",
    "\n",
    "# Load custom marker images\n",
    "img_2_year = plt.imread('./marker1.png')\n",
    "img_4_year = plt.imread('./marker1.png')\n",
    "img_total = plt.imread('./marker2.png')\n",
    "\n",
    "# Plot the lines\n",
    "plt.errorbar(df_usa_college_enroll_rate['Year'], df_usa_college_enroll_rate['2-year'], \n",
    "             yerr=df_usa_college_enroll_rate['2-year-Standard Error'], \n",
    "             marker='', color='skyblue', linewidth=2.0, label='2-year')\n",
    "\n",
    "plt.errorbar(df_usa_college_enroll_rate['Year'], df_usa_college_enroll_rate['4-year'], \n",
    "             yerr=df_usa_college_enroll_rate['4-year-Standard Error'], \n",
    "             marker='', color='lightgreen', linewidth=2.0, label='4-year')\n",
    "\n",
    "plt.errorbar(df_usa_college_enroll_rate['Year'], df_usa_college_enroll_rate['Total'], \n",
    "             yerr=df_usa_college_enroll_rate['Total-Standard Error'], \n",
    "             marker='', color='orange', linewidth=2.0, label='Total Enroll')\n",
    "\n",
    "# Add custom markers for 2-year and 4-year using AnnotationBbox\n",
    "for year, y2, y4, total in zip(df_usa_college_enroll_rate['Year'], df_usa_college_enroll_rate['2-year'], \n",
    "                               df_usa_college_enroll_rate['4-year'], df_usa_college_enroll_rate['Total']):\n",
    "    ab_2_year = AnnotationBbox(OffsetImage(img_2_year, zoom=0.035), (year, y2 + 2.8), frameon=False)\n",
    "    plt.gca().add_artist(ab_2_year)\n",
    "    ab_4_year = AnnotationBbox(OffsetImage(img_4_year, zoom=0.035), (year, y4 + 2.8), frameon=False)\n",
    "    plt.gca().add_artist(ab_4_year)\n",
    "    ab_total = AnnotationBbox(OffsetImage(img_total, zoom=0.035), (year, total + 2.8), frameon=False)\n",
    "    plt.gca().add_artist(ab_total)\n",
    "\n",
    "# Customize the plot\n",
    "plt.xlabel('Year')\n",
    "plt.ylabel('Enrollment Rate (%)')\n",
    "plt.title('The overall USA college enrollment rate for 18- to 24-year-olds', y=1.05)\n",
    "plt.legend()\n",
    "\n",
    "plt.ylim(min(df_usa_college_enroll_rate['2-year']) - 1, max(df_usa_college_enroll_rate['4-year']) + 30)\n",
    "plt.grid(False)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff6edb87",
   "metadata": {},
   "source": [
    "Explain: from year 2010 to 2021, the 2-year college enrollment dropped from 14 percent-ish to blow 10 percent, and the 4-year college enrollment maintains up and down on the 30 percent line. The overall college enrollment seems to decrease from 2018 to 2021 overtime. To our surprise, it looks like going to college is like a priviledge in the US because in the course of 10 years, the college enrollment rate is always less than 50 percent! Perhaps it is the money that is driving young people away even though the potential early career and mid career salary is descent (common sense). We couldn't find the financial pice from this same dataset. You see, if we are data scientists in the wild, it is always a good idea to first figure out what kinds of data do we want to get. That way, at least, we will have strong supporting evidences. So maybe the potential salary motivation has less effect on this indirectly.     "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c13cd156",
   "metadata": {},
   "source": [
    "We learned about the overall enrollment rates. Now a common thing to futher explore is the diversity of the data, such as the enthnicity of the population. Why? It is because USA is a very diverse country (melting pot). Thus, we present to you:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20dbba94",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up the plot\n",
    "plt.figure(figsize=(15, 5))\n",
    "\n",
    "# Calculate the width of each bar group\n",
    "bar_width = 0.35\n",
    "\n",
    "# Create positions for the bars\n",
    "positions_2010 = list(range(len(df_usa_college_enroll_rate_enthnicity)))\n",
    "positions_2021 = [pos + bar_width for pos in positions_2010]\n",
    "\n",
    "# Plot the bars for 2010 and 2021\n",
    "plt.barh(positions_2010, df_usa_college_enroll_rate_enthnicity['2010'], \n",
    "         height=bar_width, color='skyblue', alpha=0.8, label='2010')\n",
    "\n",
    "plt.barh(positions_2021, df_usa_college_enroll_rate_enthnicity['2021'], \n",
    "         height=bar_width, color='coral', alpha=0.8, label='2021')\n",
    "\n",
    "# Set y-ticks and labels\n",
    "plt.yticks([pos + bar_width / 2 for pos in positions_2010], df_usa_college_enroll_rate_enthnicity['Race'])\n",
    "\n",
    "# Customize the plot\n",
    "plt.xlabel('Enrollment Rate (%)')\n",
    "plt.ylabel('Race/Ethnicity')\n",
    "plt.title('USA College Enrollment Rates by Race/Ethnicity: 2010 vs 2021', y=1.05)\n",
    "plt.legend()\n",
    "plt.grid(False)\n",
    "# Display the plot\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d80a8875",
   "metadata": {},
   "source": [
    "Explain: from the same dataset as college enrollment rate from 2010 to 2021 ages 18-24, we see that Asian have the hightest enrollment rate of all time (60 pct +). Then comes White and Pacific islander (~ 40 pct avg). Black is just slightly less than 40 percent in avg. Perhaps Asians are more prone to be \"high college grades equals successful\"? Even that is the case, we doubt this could make an Asian student successful in education. Because focusing souly on grades will be really stressful. Whether people view college as a resource, a place to show intelligence, a factory to produce high quality labors, or a self-improvement place, these chaotic thoughts will be transformed and become \"temporarily ordered\" by schools; that is, you will learn how things work, what is best or worse, what other people are thinking, etc. <br> "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89f9fd81",
   "metadata": {},
   "source": [
    "### Malaysia College Student Mental Health dataset EDA"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ec1792f",
   "metadata": {},
   "source": [
    "Here, we will see the mental aspect in education. Enjoy :) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7217bac7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_malaysia_copy = df_malaysia_student_mental_health.copy()\n",
    "\n",
    "# See the sum counts of Age and CGPA\n",
    "grouped_age = df_malaysia_copy.groupby(['Age']).sum().reset_index()\n",
    "grouped_cgpa = df_malaysia_copy.groupby(['CGPA']).sum().reset_index()\n",
    "grouped_course = df_malaysia_copy.groupby(['Course']).sum().reset_index()\n",
    "\n",
    "# Create a list of groupings and their corresponding dataframes\n",
    "groupings = [('Age', grouped_age), ('CGPA', grouped_cgpa), ('Course', grouped_course)]\n",
    "\n",
    "# Plotting\n",
    "fig, axes = plt.subplots(nrows=1, ncols=3, figsize=(30, 8))\n",
    "\n",
    "for i, (group_name, grouped_df) in enumerate(groupings):\n",
    "    ax = axes[i]\n",
    "    stacked_cols = ['Depression', 'Anxiety', 'Panic_attack']\n",
    "    grouped_df.plot(ax=ax, x=group_name, y=stacked_cols, kind='bar', stacked=True)\n",
    "\n",
    "    ax.set_xlabel(group_name)\n",
    "    ax.set_ylabel('Count')\n",
    "    ax.set_title(f'Counts of Depression, Anxiety, and Panic Attacks by {group_name}')\n",
    "    ax.legend(title='Mental Health Condition')\n",
    "\n",
    "# Adjust layout and display the plot\n",
    "plt.grid(False)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "831e8aaa",
   "metadata": {},
   "source": [
    "Explain: the data plot shows us very interesting results! To be honest, our intuition for CGPA is: students will get really stressful with low scores, resulting depression. It turns out for students from International Islamic University Malaysia in year 2020 who has [0 - 1.99], [2.00 - 2.49], and [2.50 - 2.99] CGPA ranges have less mental health issues compared to students in the [3.00 - 3.49] and [3.50 - 4.00] ranges. It seems like to be \"successful\" in college in this case (high grades) is actually not successful in terms of mental health because students care too much to an extend grades become greater than other stuffs; otherwise, they wouldn't possibly be descently depressed, anxious, and paniced. <br>\n",
    "\n",
    "Normally, a student will go to college at the age of 18 and then graduate college at 22 (typical 4-year college life). It looks like students at the age of 18 and 19 in college is suffering a lot of mental health problems. It is because they are not \"mature enough\" to handle the college life, such as living away from home or figuring out the college course load? Students who are 20, 21, and 22 have a decrease of mental health problems. Perhaps they have a clearer path they want to work on, so less stress? The intuitive age for graduate students is 23 and 24, so we will probably see depression and anxiety here. However, without more data, we couldn't come up with more reliable testing on our assumptions.<br>\n",
    "\n",
    "When we look at the courses, bcs, bit, engineering, koe, and psychology stood out to our sight. Although we have no idea what bcs, bit, and koe means here even when we went to the university's website to check, it is expected to see engineering to be one of the anxious causing course. Do grades really make a student success, such as getting the potential salary out of the school? Of course, we all need money to survive, but does having the luxury to spend the money and time to attend college implies students are \"successful\" in the temporarily financial stable sense? Remember, we care only the education life. For outside school, the situation varies a lot lot lot. So far as we can see, the way students might see \"success\" in education v.s the way we start to see \"success\" in education is different. Maybe learning to enjoy at the moment is what makes us success. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1f88ba3",
   "metadata": {},
   "source": [
    "### USA Youth Violence History dataset EDA"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cd3964d",
   "metadata": {},
   "source": [
    "Violence is one of the least things we want to encounter during our school years. Threats will always be there; thus, being able to face threats is one way to be \"successful\" in education. The data we are using here will show you the threats statistics. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f62195c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "display(df_usa_youth_violence_history.describe())\n",
    "# Plotting\n",
    "plt.figure(figsize=(20, 8))\n",
    "\n",
    "# Line plot for each column\n",
    "plt.plot(df_usa_youth_violence_history['Year'], df_usa_youth_violence_history['Other_violent_deaths'], marker='o', label='Other Violent Deaths')\n",
    "plt.plot(df_usa_youth_violence_history['Year'], df_usa_youth_violence_history['Homicides'], marker='^', label='Homicide')\n",
    "plt.plot(df_usa_youth_violence_history['Year'], df_usa_youth_violence_history['Suicides'], marker='s', label='Suicide')\n",
    "\n",
    "\n",
    "# Add count number annotations for each point\n",
    "for index, row in df_usa_youth_violence_history.iterrows():\n",
    "    plt.annotate(f'{row[\"Other_violent_deaths\"]}', (row['Year'], row['Other_violent_deaths']), textcoords=\"offset points\", xytext=(0,10), ha='center')\n",
    "    plt.annotate(f'{row[\"Homicides\"]}', (row['Year'], row['Homicides']), textcoords=\"offset points\", xytext=(0,10), ha='center')\n",
    "    plt.annotate(f'{row[\"Suicides\"]}', (row['Year'], row['Suicides']), textcoords=\"offset points\", xytext=(0,10), ha='center')\n",
    "\n",
    "\n",
    "plt.xlabel('Year')\n",
    "plt.ylabel('Counts')\n",
    "plt.title('USA Violent Deaths Trends ages 5–18 at school: School years 1992–93 to 2018–19')\n",
    "plt.legend()\n",
    "plt.xticks(rotation=90)\n",
    "plt.grid(False)\n",
    "\n",
    "# Display the plot\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d889f35",
   "metadata": {},
   "source": [
    "Explain: Other violence Deaths are always higher than homicides and suicides. While life is wonderful (including good and bad events), behind numbers for suicide is hiding some students' pain. Maybe they weren't guided correctly. From 1992 to 1999, we see an overall decrease in homocide from 34 to 14 end points, but from 1999 to 2006, the number increases. There was a huge jump up of homocides in 2017-18 (36) and a huge jump down in 2018-19 (10). We don't believe there is any linear relationship for year and the number of Other violence Deaths, Homicides, and Suicides as the std shows the data is varied. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2af3c1ea",
   "metadata": {},
   "source": [
    "### Drug Abuse Statistics dataset EDA"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50188e3e",
   "metadata": {},
   "source": [
    "Drugs. No doubt it is a factor that can influence students negatively in education, making students fail."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "729492c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15, 4))\n",
    "plt.barh(df_drug_abuse_reasons['Reason'], df_drug_abuse_reasons['Percent'])\n",
    "plt.xlabel('Percent %')\n",
    "plt.ylabel('Reasons')\n",
    "plt.title('Top Reasons For Drug Abuse Among College Students')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f10baa0",
   "metadata": {},
   "source": [
    "Explain: it turns out 90 percent plus students do drugs because of their peers. It could indirectly show that students haven't developed a mind set of having their own logical thinking of the consequences. Curiosity and search for fun is the nature of ignorant students. Living away from family could be the factor to cause the student to do drugs, whether this is showing the disagreement, rebellion, or a-way-to-escape, the emotion here dominates the reasoning mind. Media influence is something interesting to know. Perhaps the educators failed to inform students bad things about drugs. For your own exploration, we hope you could find something interesting by extending this dataset. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "365a8a63",
   "metadata": {},
   "source": [
    "### Europe College Student Info dataset EDA"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c9b3877",
   "metadata": {},
   "source": [
    "For this dataset, we would show you a basic plot. For more visualizations on this dataset, see Step 4 below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78417d49",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_europe_copy = df_europe_students.copy()\n",
    "\n",
    "# Counting the number of classes in 'target'\n",
    "class_counts = df_europe_copy['Target'].value_counts()\n",
    "\n",
    "# Plotting a bar plot\n",
    "class_counts.plot(kind='bar', figsize=(15, 4), color='skyblue')\n",
    "\n",
    "# Adding labels and title\n",
    "plt.xlabel('Target Classes')\n",
    "plt.ylabel('Count')\n",
    "plt.title('Number of Instances in Each Target Class')\n",
    "plt.grid(False)\n",
    "\n",
    "# Display the plot\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9927b91",
   "metadata": {},
   "source": [
    "Interestingly we have a dataset of \"successful\" students based on graduation. It is also interesting to see that dropouts are about 1500 while graduate is about 2200. In the dataset, there is a Target (enrollment status) and Age at Enrollment column. Let's see what could happen if we plot them together."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2479fff0",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15, 5))\n",
    "sns.violinplot(data=df_europe_students, x='Target', y='Age at enrollment', palette='pastel')\n",
    "\n",
    "# Adding labels and title\n",
    "plt.xlabel('Enrollment Status')\n",
    "plt.ylabel('Age')\n",
    "plt.title('Violin Plot of Age by Enrollment Status')\n",
    "\n",
    "# Display the plot\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edada525",
   "metadata": {},
   "source": [
    "Explain: from the age violin plot, we see that the median is about 20 years old. People who are 17 - 20 years old are enrolled in college, and 19 - 24 years old graduated. People who are older about 25 years older tend to dropout less, and people who are younger tend to dropout more. The data is unsurprisingly skewed to younger people. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf225ea0",
   "metadata": {},
   "source": [
    "## Step 4: Model: Analysis, Hypothesis Testing, & ML"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93b85b24",
   "metadata": {},
   "source": [
    "The Europe student dataset was quite extensive originally coming with 36 features available to us. We trimmed off a few columns during Step 2 and will now build a variety of machine learning models learned throughout the course of CMSC320 to try and accurately classify an individual as a dropout, student, or graduate. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62d0d625",
   "metadata": {},
   "source": [
    "As a preliminary step, let's first verify the target label has changed from type object to integer. This was performed in Step 2 and acts as an encoding for our three categories (dropout, student, graduate). This is necessary because some algorithms depend on it and cannot operate with categorical or string-based labels. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50f3121d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_europe_students.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c40a9fd1",
   "metadata": {},
   "source": [
    "Great! Here we can observe the uniform data type of each feature and can note that the target feature is represented as an integer. Onto the next step!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a3ab0bd",
   "metadata": {},
   "source": [
    "A correlation matrix, sometimes called a heatmap, is an extremely valuable tool in data analysis and what we'll implement first. This is a square matrix that houses the correlation coefficient pairings between each feature in a particular dataset. Here, we'll be able to observe what features are positively, negatively, or not correlated meaning how they tend to change relative to each other. \n",
    "\n",
    "Let's generate a correlation matrix to get an insight into how our features behave together. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "960439df",
   "metadata": {},
   "outputs": [],
   "source": [
    "corr_matrix = df_europe_students.corr()\n",
    "\n",
    "plt.figure(figsize=(10,10))\n",
    "sns.heatmap(corr_matrix)\n",
    "plt.title('Correlation Matrix')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7a28765",
   "metadata": {},
   "source": [
    "Alright, here's the correlation matrix we wanted! Correlation matrices are also often called heatmaps and the 'temperature' indicates the correlation between features mentioned above. This is a great first insight into the inner workings of the dataframe and we can see that most features are either not correlated or negatively correlated amongst each other. In a real world setting, this is to be expected and a practical result. \n",
    "\n",
    "We've just generated a nxn correlation matrix, but what might help us more specifically is to hone in on the relationship between our features and target label specifically. Let's create a new visualization to model this. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d88c3b51",
   "metadata": {},
   "outputs": [],
   "source": [
    "target_corr = df_europe_students.corrwith(df_europe_students['Target'])\n",
    "plt.figure(figsize=(20, 5))\n",
    "sns.heatmap(target_corr.to_frame(), annot=True, cmap='coolwarm', center=0, linewidths=0.5)\n",
    "plt.title('Correlation of Target vs Features')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29bb52d4",
   "metadata": {},
   "source": [
    "Now, we have another heatmap generated that is nx1 and models the relationship between each feature and our target. Based on the results of the heatmap, we can extract the k highest correlated features to our target label.\n",
    "\n",
    "An important distinction to first make is while correlation does not imply causation, we can still reason as to why we believe these trends to be occurring with that caution in mind. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14f5cd5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the value of k (number of highest correlations to extract)\n",
    "k = 10\n",
    "\n",
    "target_corr = corr_matrix['Target']\n",
    "target_corr = target_corr.sort_values(ascending=False)[1:(k + 1)]\n",
    "\n",
    "print(\"Top\", k, \"highest correlations:\")\n",
    "print(target_corr)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c96d899",
   "metadata": {},
   "source": [
    "The output above gives us some insight into the features most strongly correlated to our target label. One might infer academic performance and grades play a strong influence into student retention. We're also able to see other trends like areas of finance (i.e. tuition, scholarship, living conditions) having some form of association. \n",
    "\n",
    "Naturally, we can reason as to why this is. If an individual is unable to perform at the needed academic level and keep up with course rigor, they're more inclined to leaving schooling. Similarly, the cost of higher education has seen a significant increasing trend across several years, making aid such as scholarships more and more in demand. If students are unable to manage and keep up with their finances, they have no choice but to forgo their studies. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90859330",
   "metadata": {},
   "source": [
    "Across CMSC320, we've learned an abundance of preliminary machine learning algorithms to perform classification, regression, clustering when needed on different datasets. Making use of Scikit-learn library and their pre-implemented models, we will run various algorithms like KNN (K-nearest neighbors), SVM (Support Vector Machines), logistic regression, decision trees, and random forests on the Europe student dataset. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07eb5747",
   "metadata": {},
   "source": [
    "To begin building said models, we need to split our dataset into training and testing subsets. The X training and testing data are feature vectors without their corresponding label. The y training and testing data are said corresponding labels for each of those observations. \n",
    "\n",
    "There are trade-offs to the size of our training/testing split but we will opt to use the conventionally standard 80/20 metric where 80% is used to train and 20% is used to test. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8926cb27",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df_europe_students.drop('Target', axis=1)\n",
    "y = df_europe_students['Target']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14961e5a",
   "metadata": {},
   "source": [
    "First, we will try K-nearest neighbors. This algorithm works by reasoning that data points with similar features tend to belong to the same classification. It finds the \"k\" nearest data points to an input with a given distance metric (i.e., Euclidean distance) and then takes the majority of the neighbors' labels to classify the input. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f0ca246",
   "metadata": {},
   "outputs": [],
   "source": [
    "knn_model = KNeighborsClassifier(n_neighbors=5)\n",
    "\n",
    "knn_model.fit(X_train, y_train)\n",
    "\n",
    "y_pred = knn_model.predict(X_test)\n",
    "\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"K-Nearest Neighbor Accuracy: {accuracy}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd945e12",
   "metadata": {},
   "source": [
    "Here, we can observe that the accuracy of KNN isn't as high as we'd like it to be. This could be due to a number of reasons such as choosing an appropriate value of k. Choosing k is rather nuanced and here we're using an arbitrary number, five. \n",
    "\n",
    "One thing to highlight is that our KNN model may be suffering from a phenomenon known as the \"curse of dimensionality.\" KNN's performance can degrade as the number of features (dimensions) in the dataset increases. In high-dimensional spaces, data points tend to be more spread out, making it difficult to find meaningful nearest neighbors. \n",
    "\n",
    "This aligns with our dataset which originally started with 36 features which we trimmed down during data processing. If we kept all 36 original features, we may have observed even worse performance from the model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e589dd8",
   "metadata": {},
   "source": [
    "Next, we will try to use logistic regression. Conventionally, logistic regression is a binary classifier algorithm. However, there is such thing as multiclass logistic regression, also known as softmax regression, which Scikit-learn defaults to when necessary. Essentially, it predicts the probability of each class for some input and uses the sofmax function to normalize the probabilities so they all add up to 1. The class with the highest probability gets predicted as the final output!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9202bf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "logreg_model = LogisticRegression(solver='liblinear')\n",
    "\n",
    "logreg_model.fit(X_train, y_train)\n",
    "\n",
    "y_pred = logreg_model.predict(X_test)\n",
    "\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Logistic Regression Accuracy: {accuracy}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77223b80",
   "metadata": {},
   "source": [
    "We see a significant improvement with our logistic regression model! One thing to note is that logistic regression tends to perform well when there is a well-balanced representation of classes amongst our examples in the dataset. As observed in Step 2, there is a rather uneven split of graduates, dropouts, and lastly enrolled students when it comes to the frequency of their class representation. If we could better balance this, perhaps if the study surveyed more people, we could see even better results. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7af6f380",
   "metadata": {},
   "source": [
    "Another model we can try is SVM (Support Vector Machines). In particular, we will use multiclass SVM to align with our three classifiers (dropout, enrolled, graduate). Standard SVM extends a hyperplane, or in laymans' terms, a divider to apply a binary classification on the dataset. In a two-dimensional space, this can be thought of as a dividing line where you either are or are not something. \n",
    "\n",
    "Multiclass SVM extends this logic and extends a hyperplane for each class as needed. Let's see how it performs!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9dbe37c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_svm(model):\n",
    "    svm_model.fit(X_train, y_train)\n",
    "\n",
    "    y_pred = svm_model.predict(X_test)\n",
    "\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "\n",
    "    print(f\"SVM Accuracy with {svm_model.kernel}: {accuracy}\")\n",
    "\n",
    "svm_model = SVC(C=1.0)\n",
    "run_svm(svm_model)\n",
    "svm_model.kernel = 'linear'\n",
    "run_svm(svm_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05716246",
   "metadata": {},
   "source": [
    "Wow, we're getting two **very** different results here! But first... what are rbf and linear?\n",
    "\n",
    "These are what we call **kernel** functions and are an important part of how SVM models operate. When a hyperplane can't be cleanly established on a dataset, we might need to apply a kernel function. This is a mathematical function that maps data points from their original feature space to a higher dimensionality. A good example of this is if we were working on a 2-dimensional feature space and our data points were jumbled up, we may not be able to draw a line that perfectly separates our different classes. A kernel function could extend these data points to a three dimensional plane such that we can more easily do this! In other words, a kernel function gives us an extra layer of flexibility when our datasets are proving slightly difficult to work with!\n",
    "\n",
    "By using a linear kernel function, we're able to observe a much better result than the default rbf kernel!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd6d8e35",
   "metadata": {},
   "source": [
    "Another fun model to try is what's called a decision tree. We can almost think of this as a flowchart where the internal nodes represent a decision our model makes based on a specific feature. We'll see this flowchart in a little bit! This leads to branches that represent outcomes or further decisions. \n",
    "At the end of the branches, you find the predicted outcome or classification based on the path taken through the tree."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb56ad77",
   "metadata": {},
   "outputs": [],
   "source": [
    "dt_model = DecisionTreeClassifier(criterion=\"entropy\")\n",
    "\n",
    "dt_model.fit(X_train,y_train)\n",
    "\n",
    "y_pred = dt_model.predict(X_test)\n",
    "\n",
    "print(f\"Accuracy Score: {accuracy_score(y_test, y_pred) * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "412ffc1f",
   "metadata": {},
   "source": [
    "Yikes! This isn't looking too good. Something to mention is that because of the 'flowchart' nature of the decision tree algorithm, there's a very important constraint we have control of called the max depth. Essentially, we'll able to control how tall our tree gets. This is especially important for large datasets like ours where we have an abundance of features and observations.\n",
    "\n",
    "Not to mention, if we let the algorithm decide on a depth of its own, we get a **really**, **really** confusing tree. Let's look at that for a minute."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b694c63d",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_cols = df_europe_students.drop('Target', axis=1).columns\n",
    "\n",
    "dot_data = export_graphviz(dt_model, out_file=None, feature_names=feature_cols, class_names=['Dropout', 'Graduate', 'Enrolled'], filled=True, rounded=True)\n",
    "graph = pydotplus.graph_from_dot_data(dot_data)\n",
    "Image(graph.create_png())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6f4ffd5",
   "metadata": {},
   "source": [
    "Not too pretty right? Let's pick a max depth that makes this easier on the eyes. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6b105b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "dt_model = DecisionTreeClassifier(criterion=\"entropy\", max_depth=3)\n",
    "\n",
    "dt_model.fit(X_train,y_train)\n",
    "\n",
    "y_pred = dt_model.predict(X_test)\n",
    "\n",
    "print(f\"Accuracy Score: {accuracy_score(y_test, y_pred) * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4496bc5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_cols = df_europe_students.drop('Target', axis=1).columns\n",
    "\n",
    "dot_data = export_graphviz(dt_model, out_file=None, feature_names=feature_cols, class_names=['Dropout', 'Graduate', 'Enrolled'], filled=True, rounded=True)\n",
    "graph = pydotplus.graph_from_dot_data(dot_data)\n",
    "Image(graph.create_png())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61e3c08a",
   "metadata": {},
   "source": [
    "By constraining the depth that we allow our decision tree to grow to, we're able to see slightly improved results. This is because the model is more likely able to make correct classifications on the test data solely based on the most highly correlated features which are chosen for splits toward the root of the tree. As our tree grows and we begin to rely on features with less correlation, our model is more prone to make errors!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c5af489",
   "metadata": {},
   "source": [
    "The last model we'll try is called Random Forest and it actually builds on decision trees! Essentially, we built n decision trees to make up a forest. Each tree in the forest works on a different scrambling of the training and test data. The final prediction from the random forest is then a combination of the predictions made by each individual tree which generally leads to a more accurate model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70e9b255",
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_model = RandomForestClassifier(n_estimators=100)\n",
    "\n",
    "rf_model.fit(X_train, y_train)\n",
    "\n",
    "y_pred = rf_model.predict(X_test)\n",
    "\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "\n",
    "print(f\"Random Forest Accuracy: {accuracy}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8c1e08d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Remove this cell for final submission, just for testing purposes and optimizing the model\n",
    "# To optimize random_state and max_depth\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from scipy.stats import randint\n",
    "\n",
    "param_dist = {\n",
    "    'random_state': randint(0, 101),  # Random values between 0 and 100\n",
    "    'max_depth': range(1, 11)  # Range of max_depth values from 0 to 10\n",
    "}\n",
    "\n",
    "random_search = RandomizedSearchCV(\n",
    "    estimator=DecisionTreeClassifier(criterion=\"entropy\"),\n",
    "    param_distributions=param_dist,\n",
    "    n_iter=100,  # Number of random combinations to try\n",
    "    scoring='accuracy',\n",
    "    cv=5,  # Number of cross-validation folds\n",
    ")\n",
    "\n",
    "random_search.fit(X_train, y_train)\n",
    "\n",
    "# Get the best hyperparameters and score\n",
    "best_params = random_search.best_params_\n",
    "best_score = random_search.best_score_\n",
    "\n",
    "print(best_params, best_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fed67e4a",
   "metadata": {},
   "source": [
    "## Step 5: Interpretation: Insight & Policy Decision"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
